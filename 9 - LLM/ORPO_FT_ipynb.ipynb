{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Library"
      ],
      "metadata": {
        "id": "RFf5b4PT40kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install --upgrade -q -U transformers\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U git+https://github.com/huggingface/trl.git"
      ],
      "metadata": {
        "id": "sRZ9qaUL42TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/huggingface/trl.git"
      ],
      "metadata": {
        "collapsed": true,
        "id": "paMOMJV-5E6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Library"
      ],
      "metadata": {
        "id": "tReL07Nn5FLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, multiprocessing\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from trl import ORPOConfig, ORPOTrainer\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "jg5ScYNf5HwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "if major_version >= 8:\n",
        "  !pip install flash-attn\n",
        "  torch_dtype = torch.bfloat16\n",
        "  attn_implementation='flash_attention_2'\n",
        "  print(\"Your GPU is compatible with FlashAttention and bfloat16.\")\n",
        "else:\n",
        "  torch_dtype = torch.float16\n",
        "  attn_implementation='eager'\n",
        "  print(\"Your GPU is not compatible with FlashAttention and bfloat16.\")"
      ],
      "metadata": {
        "id": "Afm0H56f446q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "VKiIm-JB5atD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\", split=[\"train_prefs\", \"test_prefs\"])\n",
        "dataset"
      ],
      "metadata": {
        "id": "GliubjCZ5Zd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model"
      ],
      "metadata": {
        "id": "K6hqNHxJ6KsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "# 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"
      ],
      "metadata": {
        "id": "JqL2k-6J6pO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mistralai/Mistral-7B-v0.1\""
      ],
      "metadata": {
        "id": "JQ65gHfH6J2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "id": "0wHChdGA6V-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(row):\n",
        "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
        "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
        "    return row\n",
        "\n",
        "dataset[0] = dataset[0].map(\n",
        "    process,\n",
        "    num_proc= multiprocessing.cpu_count(),\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "\n",
        "dataset[1] = dataset[1].map(\n",
        "    process,\n",
        "    num_proc= multiprocessing.cpu_count(),\n",
        "    load_from_cache_file=False,\n",
        ")\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "Pl3af4bN6WMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create BitsAndBytes Config"
      ],
      "metadata": {
        "id": "abGpBy_N889H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "SPzYazZK6WP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch_dtype,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map = {\"\":0}\n",
        ")"
      ],
      "metadata": {
        "id": "s0U88cQB6WSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "vUiIDcqv9f-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']\n",
        ")"
      ],
      "metadata": {
        "id": "KcgETbus93Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orpo_config = ORPOConfig(\n",
        "    output_dir=\"./results/\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    do_eval=False,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=2,\n",
        "    log_level=\"debug\",\n",
        "    logging_steps=20,\n",
        "    learning_rate=8e-6,\n",
        "    max_steps=20,\n",
        "    save_steps=10,\n",
        "    save_strategy='epoch',\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    beta=0.1, #beta is ORPO's lambda\n",
        "    max_length=1024,\n",
        ")"
      ],
      "metadata": {
        "id": "pj6xnfbw-HDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = ORPOTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset[0],\n",
        "    eval_dataset = dataset[1],\n",
        "    peft_config = peft_config,\n",
        "    args = orpo_config,\n",
        "    # tokenizer = tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "iooQ5Hcs-HFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "NCFJFdUJ-HJD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}